{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Install necessary packages\n",
    "\n",
    "We can install the necessary package by either running `pip install --user <package_name>` or include everything in a `requirements.txt` file and run `pip install --user -r requirements.txt`. Since we have a few packages we need to install we will use the second option.\n",
    "\n",
    "> NOTE: Do not forget to use the --user argument. It is necessary if you want to use Kale to transform this notebook into a Kubeflow pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports\n",
    "\n",
    "In this section we import the packages we need for this example. Make it a habbit to gather your imports in a single place. It will make your life easier if you are going to transform this notebook into a Kubeflow pipeline using Kale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load the data\n",
    "\n",
    "In this section, we load the data. The data come in a CSV format, thus, `pandas` offers some great options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/\"\n",
    "train_path = os.path.join(data_path, \"train.csv\")\n",
    "test_path = os.path.join(data_path, \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data processing\n",
    "\n",
    "We are now ready to preprocess the data set. This includes cleaning the dataset, imputing missing values, scaling numerical values, encoding categorical attributes etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = train_df.isnull().sum().sort_values(ascending=False)\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare the data\n",
    "\n",
    "Let us now separate the features from the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = strat_train_set.drop(\"Survived\", axis=1)\n",
    "labels = strat_train_set[\"Survived\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Most Machine Learning algorithms cannot work with missing features, so let us create a few functions to take care of them. We noticed earlier that the `Cabin`, `Age` and `Embarked` attributes have some missing values, so let us fix this. We have three options:\n",
    "\n",
    "* Get rid of the corresponding rows\n",
    "* Get rid of the whole attribute\n",
    "* Set the missing values (zero, mean, median, etc.)\n",
    "\n",
    "We will drop the `Cabin` entirely, since the most values are missing, and later impute the `Age` and `Embarked` attributes in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors.drop(\"Cabin\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Handling Text and Categorical Attributes\n",
    "\n",
    "Machine Learning algorithms prefer to work with numbers anyway, so let’s convert these text labels to numbers.\n",
    "\n",
    "`scikit-learn` provides a transformer for this task called `OneHotEncoder`, that we will use to encode the `Embarked` and `Sex` attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feature scaling\n",
    "\n",
    "One of the most important transformations we need to apply to our data is feature scaling. With few exceptions, Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales. This is the case for the Titanic data. `Fare`, `Age` and `Pclass` differ significantly. Note, that scaling the target values is generally not required.\n",
    "\n",
    "There are two common ways to get all attributes to have the same scale: min-max scaling and standardization. Min-max scaling (many people call this normalization) is quite simple: values are shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtracting the min value and dividing by the max minus the min. Scikit-Learn provides a transformer called `MinMaxScaler` for this. It has a `feature_range` hyperparameter that lets you change the range if you don’t want 0–1 for some reason.\n",
    "\n",
    "Standardization is quite different: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the variance so that the resulting distribution has unit variance. Scikit-Learn provides a transformer called `StandardScaler` for standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Putting it together\n",
    "\n",
    "As we can see, there are many data transformation steps that need to be executed in the right order. Scikit-Learn provides the Pipeline class to help with such sequences of transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attribs = [\"Pclass\", \"SibSp\", \"Parch\", \"Fare\"]\n",
    "cat_attribs = [\"Sex\", \"Embarked\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    (\"num_imputer\", num_imputer),\n",
    "    (\"std_scaler\", scaler)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline([\n",
    "    (\"cat_imputer\", cat_imputer),\n",
    "    (\"encoder\", encoder)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = ColumnTransformer([\n",
    "        ('num', num_pipeline, num_attribs),\n",
    "        ('cat', cat_pipeline, cat_attribs),\n",
    "    ])\n",
    "\n",
    "predictors_prepared = full_pipeline.fit_transform(predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Training\n",
    "\n",
    "Now that we have framed the problem, got the data and explored it, sampled a training set and a test set, and wrote our transformation pipelines to clean up and prepare our data for Machine Learning algorithms, we are ready to select and train a machine learning model. We will use cross validation to evaluate five different models:\n",
    "\n",
    "* Support Vector Machines\n",
    "* Decision Trees\n",
    "* K Nearest Neighbors\n",
    "* Random Forests\n",
    "* Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(gamma=\"auto\")\n",
    "svc.fit(predictors_prepared, labels)\n",
    "svc_scores = cross_val_score(svc, predictors_prepared, labels, scoring=\"accuracy\", cv=4, )\n",
    "print(svc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(predictors_prepared, labels)\n",
    "tree_scores = cross_val_score(tree, predictors_prepared, labels, scoring=\"accuracy\", cv=4)\n",
    "print(tree_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(predictors_prepared, labels)\n",
    "knn_scores = cross_val_score(knn, predictors_prepared, labels, scoring=\"accuracy\", cv=4)\n",
    "print(knn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(predictors_prepared, labels)\n",
    "random_forest_scores = cross_val_score(random_forest, predictors_prepared, labels, scoring=\"accuracy\", cv=4)\n",
    "print(random_forest_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression(solver=\"lbfgs\")\n",
    "logistic_regression.fit(predictors_prepared, labels)\n",
    "logistic_regression_scores = cross_val_score(logistic_regression, predictors_prepared, labels, scoring=\"accuracy\", cv=4)\n",
    "print(logistic_regression_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Evalluation\n",
    "\n",
    "We are now at the final stage of our experiment. We are ready to evaluate our algorithms using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = strat_test_set.drop([\"Survived\", \"Cabin\"], axis=1)\n",
    "y_test = strat_test_set[\"Survived\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_prepared = full_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_predictions = svc.predict(X_test_prepared)\n",
    "svc_accuracy = accuracy_score(y_test, svc_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_predictions = tree.predict(X_test_prepared)\n",
    "tree_accuracy = accuracy_score(y_test, tree_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_predictions = knn.predict(X_test_prepared)\n",
    "knn_accuracy = accuracy_score(y_test, knn_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_predictions = random_forest.predict(X_test_prepared)\n",
    "random_forest_accuracy = accuracy_score(y_test, random_forest_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_predictions = logistic_regression.predict(X_test_prepared)\n",
    "logistic_regression_accuracy = accuracy_score(y_test, logistic_regression_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svc_accuracy)\n",
    "print(tree_accuracy)\n",
    "print(random_forest_accuracy)\n",
    "print(logistic_regression_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": true,
   "docker_image": "gcr.io/arrikto/jupyter-kale:v0.5.0-47-g2427cc9",
   "experiment": {
    "id": "",
    "name": ""
   },
   "experiment_name": "",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "",
   "pipeline_name": "",
   "snapshot_volumes": true,
   "steps_defaults": [
    "label:access-ml-pipeline:true",
    "label:access-rok:true"
   ],
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/home/jovyan",
     "name": "workspace-examples-server-pbt37ljho",
     "size": 5,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
